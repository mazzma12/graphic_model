{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Probabilistic Graphical Models - HWK 3 \n",
    "\n",
    "## Paul Dufoss√© & Matthieu Mazzolini\n",
    "\n",
    "Final version of our homework (January 4 2017)\n",
    "link to the GitHub IPython Notebook : https://github.com/mazzma12/graphic_model/blob/master/PGM_HWK3_mazzolini_dufosse.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from scipy.stats import multivariate_normal as mvn\n",
    "\n",
    "from plotly.offline import init_notebook_mode, iplot\n",
    "\n",
    "init_notebook_mode()\n",
    "\n",
    "import plotly.plotly as py\n",
    "import plotly.graph_objs as go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('EMGaussian.data', delim_whitespace=True, header=None, names=['x', 'y'])\n",
    "test = pd.read_csv('EMGaussian.test', delim_whitespace=True, header=None, names=['x', 'y'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) We implement the alpha & beta recursions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lse(v):\n",
    "    #log(sum(exp)) function \n",
    "    return np.log(np.exp(v).sum())\n",
    "\n",
    "def compute_cond_proba(X, pi, mu, sigma):\n",
    "    # Compute a T.K matrix with the probability of the observation t given the states is k\n",
    "    P =np.zeros((T, K))\n",
    "    for t in range(T):\n",
    "        for k in range(K):\n",
    "            P[t, k] = mvn.pdf(X[t], mu[k], sigma[k])\n",
    "    return P \n",
    "\n",
    "def log_alpha_rec(X, A, P, pi, mu, sigma):\n",
    "    (T, p) = X.shape\n",
    "    alpha = np.ones((T,K))\n",
    "    # The LOG of the messages alpha are contained in the matrix Alpha. \n",
    "    # The t-th row corresponds to the time t\n",
    "    # The k-th column corresponds to the case where the state takes the value k\n",
    "\n",
    "    # Computation of the first alpha(q_0)\n",
    "    for k in range(K):\n",
    "        alpha[0,k] = np.log(P[0, k]) + np.log(pi[k])\n",
    "\n",
    "    for t in range(1,T):\n",
    "        for k in range(K):\n",
    "            # Alpha message formula p9 chp 12.4 of the book\n",
    "            log_proba_vec = alpha[t-1] + np.log(A[:,k])\n",
    "            m = max(log_proba_vec)\n",
    "            alpha[t, k] = m + np.log(P[t, k]) + lse(log_proba_vec - m)\n",
    "            \n",
    "    return alpha\n",
    "\n",
    "def log_beta_rec(X, A, P, pi, mu, sigma):\n",
    "    (T,p) = X.shape\n",
    "    beta = np.ones((T,K))\n",
    "    \n",
    "    # Initialization of the last time T\n",
    "    # Maybe it should be something else,\n",
    "    for k in range(K):\n",
    "        beta[T-1,k] = np.log(P[T-1, k]) + np.log(pi[k])\n",
    "    \n",
    "    for t in range(T-1)[::-1]:\n",
    "        for k in range(K):\n",
    "            # Beta message formula 12.30 p10 chp 12.4 of the book\n",
    "            # This time there is no constant term because the conditional probability\n",
    "            # depends on q_(t+1) the index of the sum\n",
    "            \n",
    "            # Therefore we have to run another loop to compute \n",
    "            # this cond probability for K values\n",
    "            cond_proba = [np.log(P[t+1, j]) for j in range(K)]\n",
    "            log_proba_vec = beta[t+1] + np.log(A[k,:]) + cond_proba\n",
    "            m = max(log_proba_vec)\n",
    "            beta[t, k] = lse(log_proba_vec-m) + m \n",
    "            \n",
    "    return beta\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can compute the probabilities : gamma(qt) and ksi(qt, qt1) (qt1 stands for q(t+1))\n",
    "gamma and ksi are the notations used in the book. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(data)\n",
    "(T, p) = X.shape\n",
    "K  =4\n",
    "pi = 1.0/4 * np.ones(4)\n",
    "\n",
    "A = np.eye(K)*(1/2-1/6) + np.ones((K,K))*1/6\n",
    "\n",
    "#we are not sure of the parameters for the previous homework \n",
    "#so we chose to use the EM estimation from the scikit algorithm\n",
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "model = GaussianMixture(n_components=4, covariance_type='full')\n",
    "model.fit(X)\n",
    "mu_ = model.means_\n",
    "sigma_ = model.covariances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_filtering(log_alpha, log_beta):\n",
    "    filtering = np.zeros((T, K))\n",
    "    for t in range(T):\n",
    "        ai = log_alpha[t, :] + log_beta[t, :]\n",
    "        max_ai = np.max(ai)\n",
    "        log_normalization = max_ai + lse(log_alpha[t, :] + log_beta[t, :] - max_ai)\n",
    "        filtering[t, :] = np.exp(log_alpha[t, :] + log_beta[t, :] - log_normalization)\n",
    "        filtering[t,:] /= np.sum(filtering[t,:])\n",
    "    return filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#p_qt \n",
    "\n",
    "P = compute_cond_proba(X, pi, mu_, sigma_)\n",
    "\n",
    "log_alpha = log_alpha_rec(X, A, P, pi, mu_, sigma_)\n",
    "log_beta = log_beta_rec(X, A, P, pi, mu_, sigma_)\n",
    "gamma = compute_filtering(log_alpha, log_beta)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_states_probability(data):\n",
    "    # Plot the conditional probability of the first 100 states fiven the observations\n",
    "    for k in range(K):\n",
    "        trace=go.Scatter(\n",
    "            x=np.arange(100),\n",
    "            y=data[0:99,k]\n",
    "        )\n",
    "        layout = go.Layout(title='probability of being at state '+str(k+1)+' at time t given all the observation',\n",
    "                          xaxis=dict(title='time'),\n",
    "                          yaxis=dict(title='probability')\n",
    "                          )\n",
    "        fig = go.Figure(data=[trace], layout=layout)\n",
    "        iplot(fig, filename=\"plot\")\n",
    "        \n",
    "plot_states_probability(gamma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the gamma matrix we compute the most probable sequence : we set 1 for the most probable state and 0 for the others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_probable (gamma):\n",
    "    # the most probable state is set to one and the others to zero\n",
    "    m_p = np.zeros((T,K))\n",
    "    for t in range(T):\n",
    "        i = np.argmax(gamma[t,:])\n",
    "        m_p[t,i] = 1\n",
    "    return m_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_states_probability(most_probable(gamma))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From alpha and gamma we compute the ksi variables and will use them in the next EM algorithm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_log_ksi(qt, qt1, t, A, P, pi, mu, sigma):\n",
    "    return log_alpha[t, qt]+np.log(P[t+1, qt1])+np.log(gamma[t, qt1])+np.log(A[qt, qt1]) -log_alpha[t+1, qt1]\n",
    "\n",
    "def compute_cooccurrence(P, A, pi, mu, sigma):\n",
    "    ksi = np.zeros(((T-1, K, K)))\n",
    "    for t in range(T-1):\n",
    "        for k in range(K):\n",
    "            for l in range(K):\n",
    "                ksi[t, k, l] = compute_log_ksi(k, l, t, A, P, pi, mu, sigma)\n",
    "    return ksi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ksi = compute_cooccurrence(P, A, pi, mu_, sigma_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) You can see the estimation equations for the EM algo in annex "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) We implement now the EM algorithm. The first E-step has already been done.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loglikelihood(X, A, P, pi, gamma):\n",
    "    most = most_probable(gamma)\n",
    "    M = np.zeros((K,K))\n",
    "    for i in range(K):\n",
    "        for j in range(K):\n",
    "            for t in range(T-1):\n",
    "                M[i,j] += most[t,i]*most[t+1,j]\n",
    "    return np.dot(most[0,:],np.log(pi)) + (np.log(A)*M).sum() + (most*np.log(P)).sum()\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def EM_hmm(X, A_iter, P_iter, pi_iter, mu_iter, sigma_iter, max_iter=30, tol=0.0000000001):\n",
    "    # EM implementation for our HMM model\n",
    "    tours = 0\n",
    "    convergence = False\n",
    "    lklh = []\n",
    "    while(convergence is False and tours<max_iter):\n",
    "\n",
    "        #E-step\n",
    "        P_iter = compute_cond_proba(X, pi_iter, mu_iter, sigma_iter)\n",
    "        log_alpha = log_alpha_rec(X, A_iter, P_iter, pi_iter, mu_iter, sigma_iter)\n",
    "        log_beta = log_beta_rec(X, A_iter, P_iter, pi_iter, mu_iter, sigma_iter)\n",
    "        gamma_iter = compute_filtering(log_alpha, log_beta)\n",
    "        ksi_iter = compute_cooccurrence(P_iter, A_iter, pi_iter, mu_iter, sigma_iter)\n",
    "        #M-step\n",
    "        for i in range(K):\n",
    "            A_iter[i,:] = ksi.sum(0)[i,:]/ksi.sum(0).sum(0)\n",
    "            pi_iter = gamma_iter[0]/gamma_iter[0].sum(0)\n",
    "        for i in range(K):\n",
    "            mu_iter[i] = np.dot(gamma[:,i], X)/gamma[:,i].sum()\n",
    "        for i in range(K):\n",
    "            temp = X-mu_iter[i]\n",
    "            sigma_temp = 0\n",
    "            for t in range(T):\n",
    "                temp = np.reshape(X[t,:] - mu_iter[i], (1, 2))\n",
    "                sigma_temp += gamma[t, i] * np.dot(temp.T, temp)             \n",
    "            sigma_temp /= np.sum(gamma[:,i])\n",
    "            sigma_iter[i] = sigma_temp\n",
    "        lklh.append(compute_loglikelihood(X, A_iter, P_iter, pi_iter, gamma_iter))\n",
    "        if (tours > 1):\n",
    "            if (np.abs(lklh[tours] - lklh[tours-1]) < tol ):\n",
    "                convergence = True\n",
    "        tours+=1\n",
    "\n",
    "    return A_iter, P_iter, pi_iter,mu_iter, sigma_iter, gamma_iter, lklh, tours\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5) We train the EM on our HMM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj = EM_hmm(X, A, P, pi, mu_, sigma_)\n",
    "[A_final, P_final, pi_final,mu_final, sigma_final, gamma_final, lklh, tours] = obj\n",
    "print(\"nombre d'iterations : \", tours)\n",
    "print(\"variation de la loglikelihood : \", lklh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6) We compute and plot the loglikelihood for the train data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comments :** We reached the maximum in 6 iterations, and we notice that the loglikelihood value doesn't improve much. This can be explained by the fact that we initiate the algorithm with the value obtained in the previous homework, which were pretty decent. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trace = go.Scatter(x=np.arange(tours), y=(lklh), name='train data')\n",
    "iplot([trace])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing the likelihood for the test data\n",
    "X_test = np.array(test)\n",
    "P_test = compute_cond_proba(X_test, pi_final, mu_final, sigma_final)\n",
    "log_alpha_test = log_alpha_rec(X_test, A_final, P_test, pi_final, mu_final, sigma_final)\n",
    "log_beta_test = log_beta_rec(X_test, A_final, P_test, pi_final, mu_final, sigma_final)\n",
    "gamma_test = compute_filtering(log_alpha_test, log_beta_test)\n",
    "likelihood_test = compute_loglikelihood(X_test, A_final, P_test, pi_final, gamma_test)\n",
    "print(\"likelihood for test data : \", likelihood_test, \"likelihood for train data : \", lklh[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comment** We observe that the loglikelihood is a bit lower on the test data than on the train data, \n",
    "which is as usual because we trained the model on the train data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7) You can see the description of Viterbi algorithm in annex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8) Now we implement a new method : the Viterbi decoding algorithm (or max-product algorithm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def viterbi(X, A, pi, mu, sigma):\n",
    "    T1=np.zeros((T, K))\n",
    "    T2=np.zeros((T, K))\n",
    "    \n",
    "    for k in range(K):\n",
    "        T1[1, k] = np.log(pi[k]) + mvn.logpdf(X[0], mu[k], sigma[k]) \n",
    "        \n",
    "    for t in range(1,T):\n",
    "        for k in range(K):\n",
    "            T1[t,k] = np.max(T1[t-1,:] + np.log(A[k,:])) + mvn.logpdf(X[t], mu[k], sigma[k])\n",
    "            T2[t,k] = np.argmax(T1[t-1,:] + np.log(A[k,:]))\n",
    "            \n",
    "    seq = np.zeros(T)\n",
    "    seq[T-1] = np.argmax(T1[T-1,:]) \n",
    "    for t in range(T-1)[::-1]:\n",
    "        seq[t-1] = T2[t, seq[t]]\n",
    "    \n",
    "    # We add 1 to have the states from 1 to 4 instead of 0 to 3   \n",
    "    seq +=1        \n",
    "    return seq\n",
    "\n",
    "viterbi_results = viterbi(X, A, pi, mu_, sigma_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We plot the result for the viterbi algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [(viterbi_results==k).astype(int) for k in range(1,5)]\n",
    "data = np.array(data).T\n",
    "print(data.shape)\n",
    "plot_states_probability(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion ** : We find the same result with the Viterbi algorithm than the ones with the filtering fonction used in question 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9) & 10) We used the gamma_test variable that we computed with the test data to compute the most likely states for test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_states_probability(gamma_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We run Viterbi algorithm with the result obtained from the test data : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viterbi_test_results = viterbi(X_test, A_final, pi_final, mu_final, sigma_final)\n",
    "data = [(viterbi_test_results==k).astype(int) for k in range(1,5)]\n",
    "data = np.array(data).T\n",
    "print(data.shape)\n",
    "plot_states_probability(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We compare the results obtained with Viterbi and the filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Difference between the two predictions\n",
    "absolute_error = np.abs((most_probable(gamma_test)-data)).sum()\n",
    "print(\" #differences between the 2 algos : \", absolute_error)\n",
    "print(\"percentage of error : \", 6/500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We had 6 errors between the Viterbi and the filtering on the test data **which is 1.2% (decent)**"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
