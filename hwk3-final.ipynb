{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Probabilistic Graphical Models - HWK 3 \n",
    "\n",
    "## Paul Dufoss√© & Matthieu Mazzolini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from scipy.stats import multivariate_normal as mvn\n",
    "\n",
    "from plotly.offline import init_notebook_mode, iplot\n",
    "\n",
    "init_notebook_mode()\n",
    "\n",
    "import plotly.plotly as py\n",
    "import plotly.graph_objs as go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('EMGaussian.data', delim_whitespace=True, header=None, names=['x', 'y'])\n",
    "test = pd.read_csv('EMGaussian.test', delim_whitespace=True, header=None, names=['x', 'y'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) We implement the alpha & beta recursions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#log(sum(exp)) function \n",
    "def lse(v):\n",
    "    return np.log(np.exp(v).sum())\n",
    "\n",
    "def compute_cond_proba(X, pi, mu, sigma):\n",
    "    P =np.zeros((T, K))\n",
    "    for t in range(T):\n",
    "        for k in range(K):\n",
    "            P[t, k] = mvn.pdf(X[t], mu[k], sigma[k])\n",
    "    return P \n",
    "\n",
    "def log_alpha_rec(X, A, P, pi, mu, sigma):\n",
    "    (T, p) = X.shape\n",
    "    alpha = np.ones((T,K))\n",
    "    # The LOG of the messages alpha are contained in the matrix Alpha. \n",
    "    # The t-th row corresponds to the time t\n",
    "    # The k-th column corresponds to the case where the state takes the value k\n",
    "\n",
    "    # Computation of the first alpha(q_0)\n",
    "    for k in range(K):\n",
    "        alpha[0,k] = np.log(P[0, k]) + np.log(pi[k])\n",
    "\n",
    "    for t in range(1,T):\n",
    "        for k in range(K):\n",
    "            # Alpha message formula p9 chp 12.4 of the book\n",
    "            log_proba_vec = alpha[t-1] + np.log(A[:,k])\n",
    "            m = max(log_proba_vec)\n",
    "            alpha[t, k] = m + np.log(P[t, k]) + lse(log_proba_vec - m)\n",
    "            \n",
    "    return alpha\n",
    "\n",
    "def log_beta_rec(X, A, P, pi, mu, sigma):\n",
    "    (T,p) = X.shape\n",
    "    beta = np.ones((T,K))\n",
    "    \n",
    "    # Initialization of the last time T\n",
    "    # Maybe it should be something else,\n",
    "    for k in range(K):\n",
    "        beta[T-1,k] = np.log(P[T-1, k]) + np.log(pi[k])\n",
    "    \n",
    "    for t in range(T-1)[::-1]:\n",
    "        for k in range(K):\n",
    "            # Beta message formula 12.30 p10 chp 12.4 of the book\n",
    "            # This time there is no constant term because the conditional probability\n",
    "            # depends on q_(t+1) the index of the sum\n",
    "            \n",
    "            # Therefore we have to run another loop to compute \n",
    "            # this cond probability for K values\n",
    "            cond_proba = [np.log(P[t+1, j]) for j in range(K)]\n",
    "            log_proba_vec = beta[t+1] + np.log(A[k,:]) + cond_proba\n",
    "            m = max(log_proba_vec)\n",
    "            beta[t, k] = lse(log_proba_vec-m) + m \n",
    "            \n",
    "    return beta\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can compute the probabilities : gamma(qt) and p_qt_qt1 (qt1 stands for q(t+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(data)\n",
    "(T, p) = X.shape\n",
    "K  =4\n",
    "pi = 1.0/4 * np.ones(4)\n",
    "\n",
    "A = np.eye(K)*(1/2-1/6) + np.ones((K,K))*1/6\n",
    "\n",
    "#we are not sure of the parameters for the previous homework \n",
    "#so we chose to use the EM estimation from the scikit algorithm\n",
    "from sklearn.mixture import gmm\n",
    "\n",
    "gm = gmm.GMM(n_components=4, covariance_type='full').fit(X)\n",
    "mu_ = gm.means_\n",
    "sigma_ = gm.covars_\n",
    "\n",
    "print(mu_)\n",
    "print(sigma_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_filtering(log_alpha, log_beta):\n",
    "    filtering = np.zeros((T, K))\n",
    "    for t in range(T):\n",
    "        ai = log_alpha[t, :] + log_beta[t, :]\n",
    "        max_ai = np.max(ai)\n",
    "        log_normalization = max_ai + lse(log_alpha[t, :] + log_beta[t, :] - max_ai)\n",
    "        filtering[t, :] = np.exp(log_alpha[t, :] + log_beta[t, :] - log_normalization)\n",
    "        filtering[t,:] /= np.sum(filtering[t,:])\n",
    "    return filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#p_qt \n",
    "\n",
    "P = compute_cond_proba(X, pi, mu_, sigma_)\n",
    "\n",
    "log_alpha = log_alpha_rec(X, A, P, pi, mu_, sigma_)\n",
    "log_beta = log_beta_rec(X, A, P, pi, mu_, sigma_)\n",
    "gamma = compute_filtering(log_alpha, log_beta)\n",
    "\n",
    "#p_qt_qt1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Plot for the first 100 points and state 1 \n",
    "#trace = \n",
    "data_plot = []\n",
    "for k in range(K):\n",
    "    trace=go.Scatter(\n",
    "        x=np.arange(100),\n",
    "        y=gamma[0:99,k]\n",
    "    )\n",
    "\n",
    "    data_plot.append(trace)\n",
    "    iplot([trace], filename=\"plot\")\n",
    "    \n",
    "iplot(data_plot, filename=\"plot\")\n",
    "   \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From alpha and gamma we compute the ksi variables and will use them in the next EM algorithm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_log_ksi(qt, qt1, t, A, pi, mu, sigma):\n",
    "    return log_alpha[t, qt]+np.log(P[t+1, qt1])+np.log(gamma(qt1))*np.log(A[qt, qt1]) -log_alpha[t+1, qt1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) You can see the estimation equations for the EM algo in annex "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4) We implement now the EM algorithm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6) We compute and plot the loglikelihoods "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7) You can see the description of Viterbi algorithm in annex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8) Now we implement a new method : the Viterbi decoding algorithm (or max-product algorithm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def viterbi(X, A, pi, mu, sigma):\n",
    "    T1=np.zeros((T, K))\n",
    "    T2=np.zeros((T, K))\n",
    "    \n",
    "    for k in range(K):\n",
    "        T1[1, k] = np.log(pi[k]) + mvn.logpdf(X[0], mu[k], sigma[k]) \n",
    "        \n",
    "    for t in range(1,T):\n",
    "        for k in range(K):\n",
    "            T1[t,k] = np.max(T1[t-1,:] + np.log(A[k,:])) + mvn.logpdf(X[t], mu[k], sigma[k])\n",
    "            T2[t,k] = np.argmax(T1[t-1,:] + np.log(A[k,:]))\n",
    "            \n",
    "    seq = np.zeros(T)\n",
    "    seq[T-1] = np.argmax(T1[T-1,:]) \n",
    "    for t in range(T-1)[::-1]:\n",
    "        seq[t-1] = T2[t, seq[t] ] \n",
    "        \n",
    "    return seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = viterbi(X, A, pi, _mu_k, _sigma)\n",
    "\n",
    "trace=go.Scatter(\n",
    "    x=np.arange(100),\n",
    "    y=res[0:99],\n",
    "    name=\"Most probable hidden states\"\n",
    ")\n",
    "\n",
    "layout=go.Layout(\n",
    "    title=\"Most probable hidden states\"\n",
    ")\n",
    "\n",
    "fig=go.Figure(\n",
    "    data=[trace],\n",
    "    layout=layout\n",
    ")\n",
    "iplot(fig, filename=\"plot for state 1\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
