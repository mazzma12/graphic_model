{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Probabilistic Graphical Models - HWK 3 \n",
    "\n",
    "## Paul Dufoss√© & Matthieu Mazzolini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from scipy.stats import multivariate_normal as mvn\n",
    "\n",
    "from plotly.offline import init_notebook_mode, iplot\n",
    "\n",
    "init_notebook_mode()\n",
    "\n",
    "import plotly.plotly as py\n",
    "import plotly.graph_objs as go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('EMGaussian.data', delim_whitespace=True, header=None, names=['x', 'y'])\n",
    "test = pd.read_csv('EMGaussian.test', delim_whitespace=True, header=None, names=['x', 'y'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) We implement the alpha & beta recursions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lse(v):\n",
    "    #log(sum(exp)) function \n",
    "    return np.log(np.exp(v).sum())\n",
    "\n",
    "def compute_cond_proba(X, pi, mu, sigma):\n",
    "    # Compute a T.K matrix with the probability of the observation t given the states is k\n",
    "    P =np.zeros((T, K))\n",
    "    for t in range(T):\n",
    "        for k in range(K):\n",
    "            P[t, k] = mvn.pdf(X[t], mu[k], sigma[k])\n",
    "    return P \n",
    "\n",
    "def log_alpha_rec(X, A, P, pi, mu, sigma):\n",
    "    (T, p) = X.shape\n",
    "    alpha = np.ones((T,K))\n",
    "    # The LOG of the messages alpha are contained in the matrix Alpha. \n",
    "    # The t-th row corresponds to the time t\n",
    "    # The k-th column corresponds to the case where the state takes the value k\n",
    "\n",
    "    # Computation of the first alpha(q_0)\n",
    "    for k in range(K):\n",
    "        alpha[0,k] = np.log(P[0, k]) + np.log(pi[k])\n",
    "\n",
    "    for t in range(1,T):\n",
    "        for k in range(K):\n",
    "            # Alpha message formula p9 chp 12.4 of the book\n",
    "            log_proba_vec = alpha[t-1] + np.log(A[:,k])\n",
    "            m = max(log_proba_vec)\n",
    "            alpha[t, k] = m + np.log(P[t, k]) + lse(log_proba_vec - m)\n",
    "            \n",
    "    return alpha\n",
    "\n",
    "def log_beta_rec(X, A, P, pi, mu, sigma):\n",
    "    (T,p) = X.shape\n",
    "    beta = np.ones((T,K))\n",
    "    \n",
    "    # Initialization of the last time T\n",
    "    # Maybe it should be something else,\n",
    "    for k in range(K):\n",
    "        beta[T-1,k] = np.log(P[T-1, k]) + np.log(pi[k])\n",
    "    \n",
    "    for t in range(T-1)[::-1]:\n",
    "        for k in range(K):\n",
    "            # Beta message formula 12.30 p10 chp 12.4 of the book\n",
    "            # This time there is no constant term because the conditional probability\n",
    "            # depends on q_(t+1) the index of the sum\n",
    "            \n",
    "            # Therefore we have to run another loop to compute \n",
    "            # this cond probability for K values\n",
    "            cond_proba = [np.log(P[t+1, j]) for j in range(K)]\n",
    "            log_proba_vec = beta[t+1] + np.log(A[k,:]) + cond_proba\n",
    "            m = max(log_proba_vec)\n",
    "            beta[t, k] = lse(log_proba_vec-m) + m \n",
    "            \n",
    "    return beta\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can compute the probabilities : gamma(qt) and ksi(qt, qt1) (qt1 stands for q(t+1))\n",
    "gamma and ksi are the notations used in the book. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(data)\n",
    "(T, p) = X.shape\n",
    "K  =4\n",
    "pi = 1.0/4 * np.ones(4)\n",
    "\n",
    "A = np.eye(K)*(1/2-1/6) + np.ones((K,K))*1/6\n",
    "\n",
    "#we are not sure of the parameters for the previous homework \n",
    "#so we chose to use the EM estimation from the scikit algorithm\n",
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "model = GaussianMixture(n_components=4, covariance_type='full')\n",
    "model.fit(X)\n",
    "mu_ = model.means_\n",
    "sigma_ = model.covariances_\n",
    "print(mu_)\n",
    "print(sigma_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_filtering(log_alpha, log_beta):\n",
    "    filtering = np.zeros((T, K))\n",
    "    for t in range(T):\n",
    "        ai = log_alpha[t, :] + log_beta[t, :]\n",
    "        max_ai = np.max(ai)\n",
    "        log_normalization = max_ai + lse(log_alpha[t, :] + log_beta[t, :] - max_ai)\n",
    "        filtering[t, :] = np.exp(log_alpha[t, :] + log_beta[t, :] - log_normalization)\n",
    "        filtering[t,:] /= np.sum(filtering[t,:])\n",
    "    return filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#p_qt \n",
    "\n",
    "P = compute_cond_proba(X, pi, mu_, sigma_)\n",
    "\n",
    "log_alpha = log_alpha_rec(X, A, P, pi, mu_, sigma_)\n",
    "log_beta = log_beta_rec(X, A, P, pi, mu_, sigma_)\n",
    "gamma = compute_filtering(log_alpha, log_beta)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_states_probability(data):\n",
    "    # Plot the conditional probability of the first 100 states fiven the observations\n",
    "    for k in range(K):\n",
    "        trace=go.Scatter(\n",
    "            x=np.arange(100),\n",
    "            y=data[0:99,k]\n",
    "        )\n",
    "        layout = go.Layout(title='probability of being at state '+str(k+1)+' at time t given all the observation',\n",
    "                          xaxis=dict(title='time'),\n",
    "                          yaxis=dict(title='probability')\n",
    "                          )\n",
    "        fig = go.Figure(data=[trace], layout=layout)\n",
    "        iplot(fig, filename=\"plot\")\n",
    "        \n",
    "plot_states_probability(gamma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the gamma matrix we compute the most probable sequence : we set 1 for the most probable state and 0 for the others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_probable (gamma):\n",
    "    m_p = np.zeros((T,K))\n",
    "    for t in range(T):\n",
    "        i = np.argmax(gamma[t,:])\n",
    "        m_p[t,i] = 1\n",
    "    return m_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_states_probability(most_probable(gamma))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From alpha and gamma we compute the ksi variables and will use them in the next EM algorithm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_log_ksi(qt, qt1, t, A, P, pi, mu, sigma):\n",
    "    return log_alpha[t, qt]+np.log(P[t+1, qt1])+np.log(gamma[t, qt1])+np.log(A[qt, qt1]) -log_alpha[t+1, qt1]\n",
    "\n",
    "def compute_cooccurrence(P, A, pi, mu, sigma):\n",
    "    ksi = np.zeros(((T-1, K, K)))\n",
    "    for t in range(T-1):\n",
    "        for k in range(K):\n",
    "            for l in range(K):\n",
    "                ksi[t, k, l] = compute_log_ksi(k, l, t, A, P, pi, mu, sigma)\n",
    "    return ksi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ksi = compute_cooccurrence(P, A, pi, mu_, sigma_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) You can see the estimation equations for the EM algo in annex "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4) We implement now the EM algorithm. The first E-step has already been done.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loglikelihood(X, A, P, pi, gamma):\n",
    "    most = most_probable(gamma)\n",
    "    M = np.zeros((K,K))\n",
    "    for i in range(K):\n",
    "        for j in range(K):\n",
    "            for t in range(T-1):\n",
    "                M[i,j] += most[t,i]*most[t+1,j]\n",
    "    return np.dot(most[0,:],np.log(pi)) + (np.log(A)*M).sum() + (most*np.log(P)).sum()\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_loglikelihood(X, A, P, pi, gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" A_iter = np.zeros((K,K))\n",
    "for i in range(K):\n",
    "    A_iter[i,:] = ksi.sum(0)[i]/ksi.sum(0).sum(0)\n",
    "    \n",
    "pi_iter = pi\n",
    "gamma_iter = gamma\n",
    "mu_iter = np.zeros((K, p))\n",
    "for i in range(K):\n",
    "    mu_iter[i] = np.dot(gamma[:,i], X)/gamma[:,i].sum()\n",
    "sigma_iter = np.zeros(((K, p, p)))\n",
    "for i in range(K):\n",
    "    temp = X-mu_iter[i]\n",
    "    for t in range(T):\n",
    "        temp[t] *= gamma[t,i]\n",
    "    sigma_iter[i] = np.dot(temp.T, X-mu_iter[i]) \"\"\"\n",
    "\n",
    "def EM_hmm(X, A_iter, P_iter, pi_iter, mu_iter, sigma_iter):\n",
    "    \n",
    "    tours = 0\n",
    "    max_iter = 30 \n",
    "    tolerance = 0.01\n",
    "    convergence = False\n",
    "    lklh = 0\n",
    "\n",
    "    while(convergence is False and tours<max_iter):\n",
    "\n",
    "        #E-step\n",
    "        P_iter = compute_cond_proba(X, pi_iter, mu_iter, sigma_iter)\n",
    "        log_alpha = log_alpha_rec(X, A_iter, P_iter, pi_iter, mu_iter, sigma_iter)\n",
    "        log_beta = log_beta_rec(X, A_iter, P_iter, pi_iter, mu_iter, sigma_iter)\n",
    "        gamma_iter = compute_filtering(log_alpha, log_beta)\n",
    "        ksi_iter = compute_cooccurrence(P_iter, A_iter, pi_iter, mu_iter, sigma_iter)\n",
    "\n",
    "        #M-step\n",
    "        for i in range(K):\n",
    "            A_iter[i,:] = ksi.sum(0)[i,:]/ksi.sum(0).sum(0)\n",
    "            pi_iter = gamma_iter[0]/gamma_iter[0].sum(0)\n",
    "        for i in range(K):\n",
    "            mu_iter[i] = np.dot(gamma[:,i], X)/gamma[:,i].sum()\n",
    "        for i in range(K):\n",
    "            temp = X-mu_iter[i]\n",
    "            for t in range(T):\n",
    "                temp[t] *= gamma[t,i]\n",
    "            sigma_iter[i] = np.dot(temp.T, X-mu_iter[i])\n",
    "\n",
    "        tours+=1\n",
    "        lklh_old = lklh\n",
    "        lklh = compute_loglikelihood(X, A_iter, P_iter, pi_iter, gamma_iter)\n",
    "        print(lklh)\n",
    "        if (lklh - lklh_old < 0 ):\n",
    "            convergence = True\n",
    "        \n",
    "    return A_iter, P_iter, pi_iter,mu_iter, sigma_iter, gamma_iter, lklh, tours\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj = EM_hmm(X, A, P, pi, mu_, sigma_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[A_final, P_final, pi_final,mu_final, sigma_final, gamma_final, lklh, tours] = obj\n",
    "obj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6) We compute and plot the loglikelihoods "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7) You can see the description of Viterbi algorithm in annex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8) Now we implement a new method : the Viterbi decoding algorithm (or max-product algorithm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def viterbi(X, A, pi, mu, sigma):\n",
    "    T1=np.zeros((T, K))\n",
    "    T2=np.zeros((T, K))\n",
    "    \n",
    "    for k in range(K):\n",
    "        T1[1, k] = np.log(pi[k]) + mvn.logpdf(X[0], mu[k], sigma[k]) \n",
    "        \n",
    "    for t in range(1,T):\n",
    "        for k in range(K):\n",
    "            T1[t,k] = np.max(T1[t-1,:] + np.log(A[k,:])) + mvn.logpdf(X[t], mu[k], sigma[k])\n",
    "            T2[t,k] = np.argmax(T1[t-1,:] + np.log(A[k,:]))\n",
    "            \n",
    "    seq = np.zeros(T)\n",
    "    seq[T-1] = np.argmax(T1[T-1,:]) \n",
    "    for t in range(T-1)[::-1]:\n",
    "        seq[t-1] = T2[t, seq[t]]\n",
    "    \n",
    "    # We add 1 to have the states from 1 to 4 instead of 0 to 3   \n",
    "    seq +=1        \n",
    "    return seq\n",
    "\n",
    "viterbi_results = viterbi(X, A, pi, mu_, sigma_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [(viterbi_results==k).astype(int) for k in range(1,5)]\n",
    "data = np.array(data).T\n",
    "print(data.shape)\n",
    "plot_states_probability(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
